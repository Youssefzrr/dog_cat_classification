{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734e4a12-a21e-476e-81df-e3570c82309d",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0bbdbfd-c6d0-49b7-b193-594d7e8153fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784f421a-a343-4446-b4cc-95145dd2854e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33385292-56bb-46a8-b80b-1a6b56c7ffed",
   "metadata": {},
   "source": [
    "# Part1: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07758751-cbfd-4a13-bac1-0823d8c96145",
   "metadata": {},
   "source": [
    "### Processing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3e7bc3-c194-4415-9642-8b07b0350099",
   "metadata": {},
   "outputs": [],
   "source": [
    "traind_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range= 0.2,\n",
    "    zoom_range= 0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315e8ff3-f6e1-4022-bca0-93a45e718c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = traind_datagen.flow_from_directory(\n",
    "    'training_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7670b6f-7e17-476c-ba00-2aac40d623f2",
   "metadata": {},
   "source": [
    "### Processing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f937cc9-208b-4c40-8468-5c7305830261",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89ed59b7-f2a0-44eb-8fb2-37334dcdb07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "    'test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f043740-c380-4f00-ba03-775cb1a54f18",
   "metadata": {},
   "source": [
    "# Part2: Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb6b2c-bd66-441c-a8ea-fb18fe648e25",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e32cecb1-054c-4cce-98e4-52bc9e8b004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc6577-271a-4253-8ec4-e0e6944ad122",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84104e1f-ed09-4ee2-9f4c-e1e3ee1442ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3 , activation=\"relu\", input_shape=(64, 64, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b8ac9-9463-4761-8c0a-6f405b20b0ae",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a0fc542-e12f-44c0-b998-f83362c8894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size =2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd76793-6584-4ea5-aa14-101216c960ed",
   "metadata": {},
   "source": [
    "### Adding 2nd layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dcfbb5e-9edf-40e2-9bc3-09db84f5da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3 , activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size =2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a3e57-5b41-4998-ae13-7d43cd32076f",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e7cd5b2-d102-42cd-903e-325c8961acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d3970-08ba-4608-bcdf-b2e8a47be65d",
   "metadata": {},
   "source": [
    "### Step 4 - Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5552e97d-8d4a-4dbb-8eca-6ca0ea3e63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc2926-21ce-4f50-8a3a-ca263d78115a",
   "metadata": {},
   "source": [
    "### Step 5 - Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a050cb6-e196-450d-9db1-ef7faee83650",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043813e-37a6-453d-bfbe-a0b60c6dc45b",
   "metadata": {},
   "source": [
    "# Part3: Training CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c8697-f692-44a7-b071-0f0b8e525d4f",
   "metadata": {},
   "source": [
    "### Compiling CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e41a9fa1-fa1b-4f13-8921-b81e7840dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdbaf5-3b15-4268-8257-8b9342c24a17",
   "metadata": {},
   "source": [
    "### Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f305d265-07ff-4bd0-b793-b0dfda74656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606ms/step - accuracy: 0.5266 - loss: 0.6938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 689ms/step - accuracy: 0.5266 - loss: 0.6937 - val_accuracy: 0.5960 - val_loss: 0.6805\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 253ms/step - accuracy: 0.6003 - loss: 0.6644 - val_accuracy: 0.5915 - val_loss: 0.6676\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 288ms/step - accuracy: 0.6696 - loss: 0.6069 - val_accuracy: 0.7115 - val_loss: 0.5773\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.7103 - loss: 0.5614 - val_accuracy: 0.7350 - val_loss: 0.5350\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 222ms/step - accuracy: 0.7446 - loss: 0.5222 - val_accuracy: 0.7590 - val_loss: 0.5065\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 247ms/step - accuracy: 0.7600 - loss: 0.4845 - val_accuracy: 0.7695 - val_loss: 0.4962\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 315ms/step - accuracy: 0.7855 - loss: 0.4609 - val_accuracy: 0.7815 - val_loss: 0.4618\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 266ms/step - accuracy: 0.7939 - loss: 0.4363 - val_accuracy: 0.7850 - val_loss: 0.4755\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 186ms/step - accuracy: 0.8005 - loss: 0.4228 - val_accuracy: 0.7860 - val_loss: 0.4674\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 201ms/step - accuracy: 0.8228 - loss: 0.4078 - val_accuracy: 0.8005 - val_loss: 0.4431\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 265ms/step - accuracy: 0.8200 - loss: 0.3890 - val_accuracy: 0.8045 - val_loss: 0.4585\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 285ms/step - accuracy: 0.8501 - loss: 0.3512 - val_accuracy: 0.8090 - val_loss: 0.4419\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 289ms/step - accuracy: 0.8427 - loss: 0.3560 - val_accuracy: 0.8090 - val_loss: 0.4458\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - accuracy: 0.8559 - loss: 0.3289 - val_accuracy: 0.8025 - val_loss: 0.4706\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 284ms/step - accuracy: 0.8579 - loss: 0.3204 - val_accuracy: 0.8125 - val_loss: 0.4466\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.8617 - loss: 0.3123 - val_accuracy: 0.7740 - val_loss: 0.5148\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 261ms/step - accuracy: 0.8708 - loss: 0.2882 - val_accuracy: 0.7915 - val_loss: 0.5211\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 303ms/step - accuracy: 0.8784 - loss: 0.2845 - val_accuracy: 0.7955 - val_loss: 0.5252\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 239ms/step - accuracy: 0.8930 - loss: 0.2581 - val_accuracy: 0.7915 - val_loss: 0.5407\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - accuracy: 0.9000 - loss: 0.2484 - val_accuracy: 0.8095 - val_loss: 0.5019\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 281ms/step - accuracy: 0.9137 - loss: 0.2156 - val_accuracy: 0.8170 - val_loss: 0.5009\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 210ms/step - accuracy: 0.9098 - loss: 0.2291 - val_accuracy: 0.8090 - val_loss: 0.5233\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - accuracy: 0.9148 - loss: 0.2087 - val_accuracy: 0.8125 - val_loss: 0.5616\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 206ms/step - accuracy: 0.9249 - loss: 0.1973 - val_accuracy: 0.8025 - val_loss: 0.5946\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 201ms/step - accuracy: 0.9295 - loss: 0.1796 - val_accuracy: 0.8145 - val_loss: 0.5749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2307d285650>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data=test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc7695-6f36-4654-911d-71e32625ff31",
   "metadata": {},
   "source": [
    "# Part4: Making single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "594f3d3e-96ce-4796-8dee-d9019a0c7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f7e1538-79e2-4bba-b090-20bbe7a8df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    # Load the image\n",
    "    test_image = image.load_img(path, target_size=(64, 64))\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = image.img_to_array(test_image)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Make a prediction\n",
    "    result = cnn.predict(img_array)\n",
    "    \n",
    "    # Get the class indices\n",
    "    class_indices = training_set.class_indices\n",
    "    \n",
    "    # Determine the prediction\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'dog'\n",
    "    else:\n",
    "        prediction = 'cat'\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13cd9fbf-47c1-4d03-ab99-ccb9e7c466b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "dog\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(predict('single_prediction/cat_or_dog_1.jpg'))  # This is a dog image\n",
    "print(predict('single_prediction/cat_or_dog_2.jpg'))  # This is a cat image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac83d3-b2d2-42e9-8e11-283bf508d49f",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6051e64a-0c97-4467-9e1a-e06e3200c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('../model/cat_or_dog.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2258be-cb4a-45d9-b5a1-e080f856ca32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
